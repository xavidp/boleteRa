---
title: "BoleteRa Crosstalk"
author: "Xavier de Pedro"
date: "15 d’octubre de 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r install and load required packages if needed, echo=FALSE}
if (!require("devtools")) install.packages("devtools"); library(devtools)

# Packages for Data manipulation
if (!require("readODS")) install.packages("readODS"); library(readODS)
if (!require("readxl")) install.packages("readxl"); library(readxl)
if (!require("readr")) install.packages("readr"); library(readr)
if (!require("XML")) install.packages("XML"); library(XML)
if (!require("openxlsx")) install.packages("openxlsx"); library(openxlsx)
if (!require("dplyr")) install.packages("dplyr"); library(dplyr)
if (!require("tidyr")) install.packages("tidyr"); library(tidyr)

# Packages for Html5 Output manipulation
# Add package widget frame so that we attempt to get responsive iframes produced by htmlwidgets
# See: https://github.com/bhaskarvk/widgetframe
if(!require(widgetframe)){ install.packages("widgetframe") }; library(widgetframe)
if(!require(htmlwidgets)){ install.packages("htmlwidgets") }; library(htmlwidgets) 
# Add webshot to allow making screenshots from htmlwidgets for printed versions of reports
if(!require(webshot)){ install.packages("webshot") }; library(webshot)
# Install also phantomjs thorugh webshot as a required dependency (only once)
#webshot::install_phantomjs(baseURL = "https://bitbucket.org/ariya/phantomjs/downloads/")

# Note: some system packages are needed for tmap to install properly in Ubuntu 16.04 systems
# and for some of them you may need to add an extra repository (confirmed upto Ubuntu 16.04)
# sudo add-apt-repository -y ppa:opencpu/jq
# sudo add-apt-repository -y ppa:ubuntugis/ubuntugis-unstable
# sudo apt update
# sudo apt install libudunits2-0 libudunits2-dev libjq-dev libprotobuf-dev protobuf-compiler libgdal20

# Packages for Maps
if (!require("tmap")) install.packages("tmap")
if (!require("leaflet")) install_github("rstudio/leaflet"); library(leaflet)
if (!require("leaflet.minicharts")) install.packages("leaflet.minicharts"); library(leaflet.minicharts)
if (!require("geonames")) install.packages("geonames"); library(geonames)
if (!require("ggmap")) install.packages("ggmap"); library(ggmap)

# Packages for Interactive Tables
if (!require("DT")) install.packages("DT"); library(DT)
if (!require(rpivotTable)){ install.packages("rpivotTable") }; require(rpivotTable)

# Packages for Dashboard-like UI to display results 
# See https://rstudio.github.io/crosstalk/using.html
if (!require("crosstalk")) install_github("rstudio/crosstalk"); library(crosstalk)
# Test adding an easy interface to allow changing htmlwidget on the go with manipulatewidget
# See: https://github.com/rte-antares-rpackage/manipulateWidget
if (!require("manipulateWidget")) install.packages("manipulateWidget"); library(manipulateWidget) 

```

## Read source data

```{r Read source data, echo=FALSE}
  # =======================================================
  # First approach: Use readODS package to keep data easily in a Visual Spreadsheet in LibreOffice
  # =======================================================
  # 

# Define some paths
path.html     <- "html"
path.data     <- file.path("~", "Dropbox", "Bolets", "zamiaDroid", "Citations", "00_data")
path.results  <- file.path("~", "Dropbox", "Bolets", "zamiaDroid", "Citations", "00_results")
path.photos   <- file.path("~", "Sync", "BQa5_Zamiadroid", "Photos")

# =======================================================
# Former 2nd approach: Use readr to import a csv file
# =======================================================
# 

# ### Dataset
# #read datasets
setwd(path.data)
# #data.file <- "20140000_20170900_bolets_Zamia_TAB_Xavier_dePedro.csv"
# data.file <- "20140000_20171014_bolets_Zamia_TAB.csv"
temp = list.files(path=path.data, pattern="*.tab", full.names = T)
myfiles = lapply(temp, readr::read_tsv)

#Combine files. derived from https://stackoverflow.com/a/11433532
md.orig <- dplyr::bind_rows(myfiles) 
md <- md.orig
```

## Add unique CitationId (cid) and remove duplicates
```{r Add unique CitationId (cid)}
cid <- format.Date(md$Date, "%y%m%d_%H%M%S")
md <- dplyr::bind_cols(list(data.frame(cid), md)) 
tail(md)
md <- md %>% 
  arrange(cid)
tail(md)
# Mostra duplicats
# Return all duplicated elements
md.dups <- md %>%
  filter(duplicated(md) | duplicated(md, fromLast = TRUE))

# Remove duplicates with efficient function (distinct from dplyr)
# Remove duplicated rows based on Sepal.Length
md <- md %>% distinct(cid, .keep_all = TRUE)

rbind(head(md.orig),tail(md.orig))
rbind(head(md),tail(md))
```

## Clean path from fotos

```{r}
table.foto.paths <- table(dirname(unlist(strsplit(as.character(md$foto), "; "))))
table.foto.paths

for (ii in 1:length(names(table.foto.paths))) {
  md$foto <- gsub(paste0(names(table.foto.paths)[ii], "/"), "", md$foto, fixed="T")
}
```
## Check that photo exists

```{r Check that photo exists}
# Check that photo exists and how many are there
pic.count <- list()
for (ii in 1:length(md$foto)){
  pic.count[[ii]] <- table(file.exists(file.path(path.photos, strsplit(md$foto, "; ")[[ii]])))["TRUE"]
}
pic.count.df <- do.call(rbind.data.frame, pic.count)
colnames(pic.count.df) <- "n.pics"
table(pic.count.df$n.pics)

md <- md %>% 
  mutate(pics = pic.count.df$n.pics)

md.missing.pics <- md[which(is.na(md$pics)),]
md.missing.pics
#table(file.exists(file.path(path.photos, md$foto)))

```

## Tidy data set

```{r Tidy data set, echo=FALSE}
# =======================================================
# Tidy data set
# =======================================================
# SPlit lat and long in two columns from the combined values in one column produced by zamiaDroid
library(stringr)
md2 <- cbind(md, data.frame(str_split_fixed(md$CitationCoordinates, " ", 2)))
names(md2)[ncol(md2)-1] <-  "Latitude"
names(md2)[ncol(md2)] <-  "Longitude"
md2$Latitude <- as.numeric(levels(md2$Latitude))[md2$Latitude]
md2$Longitude <- as.numeric(levels(md2$Longitude))[md2$Longitude]
md <- md2
rm(md2)

# Clean up data and fix oddities for R
md$Date <- as.POSIXct(strptime(md$Date, "%Y-%m-%d %H:%M:%S"))
#d$Pendent[is.na(md$Pendent)] <- "99"
md$Abundància <- gsub("[^0-9]", "", md$Abundància) 
md$Abundància <- as.numeric(md$Abundància)
md$Pendent <- gsub("[^0-9]", "", md$Pendent) 
md$Pendent <- as.numeric(md$Pendent)
md$`nom vulgar (comestible)` <- as.factor(md$`nom vulgar (comestible)`)
md$`nom vulgar (no comestible)` <- as.factor(md$`nom vulgar (no comestible)`)

rbind(head(md),
      tail(md))
```

## Remove some secondary columns
```{r Remove some secondary columns}
md <- md %>%
  select(-CitationCoordinates,
         -SecondaryCitationCoordinates,
         -X,
         -Y,
         -"poligon?",
         -Autor
         ) 
md <- md %>%
  unite(Descripcio_Lloc, "descripció lloc","Descripció del lloc")
md$Descripcio_Lloc <- gsub("_NA", "", md$Descripcio_Lloc)
md$Descripcio_Lloc <- gsub("NA_", "", md$Descripcio_Lloc)

```

## Add Missing Altitudes and Toponimc Location info

```{r Add Missing Altitudes}
fname.extra.info <- paste0("_120000_", format(Sys.Date(), "%y%m"), "00_extra_info.csv")

if (file.exists(file.path(path.data, fname.extra.info))) {
 extra.info.tmp <- readr::read_csv(file.path(path.data, fname.extra.info))  
} else {
  extra.info.tmp <- data.frame("NA")
  colnames(extra.info.tmp)[1] <- "cid"
}

# Remove redundant columns  from extra.info.tmp
extra.info.tmp <- extra.info.tmp %>%
  select(-Latitude,
         -Longitude)
# Merge alçada from csv file to new md read from csv filers from disk
md.new <- dplyr::anti_join(md, extra.info.tmp, by="cid")
md.old <- dplyr::inner_join(md, extra.info.tmp, by="cid")

if (nrow(md.new) > 0 ) {
  # Geo altitude for the missing data points
  # From geonames
  
  # See https://stackoverflow.com/a/8975851
  # You can use the package that looks up from geonames, and get the value from the srtm3 digital elevation model:
  # 
  # > require(geonames)
  # > GNsrtm3(54.481084,-3.220625)
  #   srtm3       lng      lat
  # 1   797 -3.220625 54.48108
  # 
  # or the gtopo30 model:
  # 
  # > GNgtopo30(54.481084,-3.220625)
  #   gtopo30       lng      lat
  # 1     520 -3.220625 54.48108
  # 
  # geonames is on CRAN so install.packages("geonames") will get it.
  #No geonamesUsername set. See http://geonames.wordpress.com/2010/03/16/ddos-part-ii/ and set one with options(geonamesUsername="foo") for some services to work
  options(geonamesUsername="xavidp") 
  #GNsrtm3(54.481084,-3.220625)
  
  # Define empty lists to store temporary information retrieved by geonames servers 
  alcada_srtm3 <- list()
  alcada_gtopo30 <- list()
  lloc <- list()
  data.xml <- list()
  data <- list()
  
  # For each step [i], keep CitationId (md$cid[i]) also just in case
  for (i in 1:nrow(md.new)) {
    # Comprova si tenim valor de Latitude o no tenim coordenades
    if (!is.na(md.new$Latitude[i])) {
      # Get altitude info from elevation models
      alcada_srtm3[[i]] <- c(md.new$cid[i], GNsrtm3(md.new$Latitude[i], md.new$Longitude[i]))
      alcada_gtopo30[[i]] <- c(md.new$cid[i], GNgtopo30(md.new$Latitude[i], md.new$Longitude[i]))
      # Get Location info
      data.xml[[i]] <- xmlParse(paste0("http://api.geonames.org/extendedFindNearby?lat=", # long (extendedFindNearby)
                                 md.new$Latitude[i], "&lng=", # long (extendedFindNearby)
                                 md.new$Longitude[i], "&username=xavidp") # long (extendedFindNearby)
                                ) # long (extendedFindNearby)
      data[[i]] <- xmlToList(data.xml[[i]]) # long (extendedFindNearby)
    
      # Recupera les dades d'interès, amb coordenades primer per garantir que tenim dataframe sencer
      # fins i tot en casos en que no obtenim res com a noms de lloc
      lloc[[i]] <- c(md.new$cid[i],
                     md.new$Latitude[i],
                     md.new$Longitude[i],
                     data[[i]][length(data[[i]])-2]$geoname$toponymName, # Província - en català
                     data[[i]][length(data[[i]])-1]$geoname$toponymName, # Ciutat
                     data[[i]][length(data[[i]])  ]$geoname$toponymName  # Nom toponímic (lloc)
                     )
      } else {
        # No tenim coordenades per a aquesta citació, per la raó que sigui
      alcada_srtm3[[i]]   <- c(md.new$cid[i], "-", NA, NA)
      alcada_gtopo30[[i]] <- c(md.new$cid[i], "-", NA, NA)
      lloc[[i]]           <- c(md.new$cid[i], "-", ".", NA, NA, NA)
    }
  }
  
  # convert list of results into data.frame
  alcada_srtm3.df   <- do.call(rbind.data.frame, alcada_srtm3)
  alcada_gtopo30.df <- do.call(rbind.data.frame, alcada_gtopo30)
  lloc.df           <- do.call(rbind.data.frame, lloc)
  if (length(colnames(lloc.df))>3) {
    colnames(lloc.df) <- c("cid", "Lat", "Lon", "Província", "Ciutat", "Nom_toponímic")
  } else {
    colnames(lloc.df) <- c("cid", "Lat", "Lon")
    lloc.df <- lloc.df %>%
      mutate("Província"=NA,
             "Ciutat"=NA,
             "Nom_toponímic"=NA
             )
  }

  }

# Add columns with extra info to md.new df
md.new <- md.new %>%
  mutate(alcada_srtm3=alcada_srtm3.df$srtm3,
         alcada_gtopo30=alcada_gtopo30.df$gtopo30,
         "Província"=lloc.df$Província,
         "Ciutat"=lloc.df$Ciutat,
         "Nom_toponímic"=lloc.df$Nom_toponímic
         )

# Combine md.old rows with md.new rows to make the updated md
md <- rbind(md.old, md.new)

#lloc.df %>% filter(str_detect(Ciutat, "[a-zA-Z]"))
#lloc.df %>% filter(str_detect(Ciutat, "[:digit:]"))


# Save partial results to prevent overquerying the geonames server
extra.info <- md %>% 
  filter(str_detect(Ciutat, "[a-zA-Z]")) %>%
  select(cid,
         Latitude,
         Longitude,
         alcada_srtm3,
         alcada_gtopo30,
         "Província",
         "Ciutat",
         "Nom_toponímic"
         )

# Desa arxiu amb extra info a disc per poder reemprar la info posteriors vegades
readr::write_csv(extra.info, file.path(path.data, fname.extra.info))

# Or this: https://stackoverflow.com/a/41773871
# to use googleway package with an api key
```

## Chunk to debug dummy data - temporary urls for fetching nearest city info

```{r Chunk to debug dummy data - temporary urls for fetching nearest city info}
if (FALSE) {
  # Set als FALSE as it's been implemented in previous chunks
  # See: https://stackoverflow.com/a/42320195
  
  # Here is a quick solution to reverse geocoding in ggmap:
  # 
  #library(ggmap)
  # 
  # > coords
  #         lon      lat
  # 1  37.61730 55.75583
  # 2 116.40739 39.90421
  # 3 -77.03687 38.90719
  # 
  # res <- lapply(with(coords, paste(lat, lon, sep = ",")), geocode, output = "more")
  #res <- lapply(with(md, paste(Latitude, Longitude, sep = ",")), geocode, output = "more")
  
  # 
  # 
  # > transform(coords, city = sapply(res, "[[", "locality"))
  #         lon      lat       city
  # 1  37.61730 55.75583     Moskva
  # 2 116.40739 39.90421    Beijing
  # 3 -77.03687 38.90719 Washington
  
  #Another approachm using geonames
  require(XML)
  #data <- xmlParse("http://api.geonames.org/findNearby?lat=41.804690&lng=2.100230&username=xavidp")
  # Coordinates from Castellterçol
  if (FALSE) {
    data2 <- xmlParse("http://api.geonames.org/findNearby?lat=41.65926673&lng=2.52292308&username=xavidp")
    xml_data2 <- xmlToList(data2)
    xml_data2$geoname$toponymName
  }
  if (FALSE) {
    
    # Extended find nearby
    data3 <- xmlParse("http://api.geonames.org/extendedFindNearby?lat=41.65926673&lng=2.52292308&username=xavidp")
    data3 <- xmlParse("http://api.geonames.org/extendedFindNearby?lat=41.94516782&lng=1.33559261&username=xavidp")
    xml_data3 <- xmlToList(data3)
    xml_data3[[length(xml_data3)]] # Mostra el darrer nom de geonames
    
    lloc <- list
    lloc <- c(xml_data3[[length(xml_data3)-2]]$toponymName, # Província - en català
              xml_data3[[length(xml_data3)-1]]$toponymName, # Ciutat
              xml_data3[[length(xml_data3)]]$toponymName # Nom toponímic (lloc)
    ) 
  }
  
  
  
  #xml_data <- xmlToList(data)
  #xml_data$geoname$toponymName
  #[1] "Comarca del Moianès"
  #xml_data$geoname$name
  #[1] "Comarca del Moianès"
  
  # Alternative way of reverse geocoding: using Here Maps and account
  #https://developer.here.com/api-explorer/rest/geocoder/reverse-geocode
}
```

## Get historical wheather data (rainfall, temp)

```{r Get historical wheather data (rainfall, temp) through WU}
# WU - Weather Underground
# ----------------------------------
# https://www.wunderground.com/history/
# They offer historical weather data for many weather stations, also in SPain. Not all of them but just a few have rainfall data, apparently (in most cases of stations monthly rainall data shows zero mm of rainfall when it did rain indeed. Sabadaell, Moia, Tona, Cerdanyola, etc. A weather station that seems to carry rainfal data effectively is "Barcelona Airport El Prat", shown also when selecting Sant Cugat, for insstance, as wheater station )
#
# rwunderground: R Interface to Weather Underground API
# 
# Tools for getting historical weather information and forecasts from wunderground.com. Historical weather and forecast data includes, but is not limited to, temperature, humidity, windchill, wind speed, dew point, heat index. Additionally, the weather underground weather API also includes information on sunrise/sunset, tidal conditions, satellite/webcam imagery, weather alerts, hurricane alerts and historical high/low temperatures.
# https://cran.r-project.org/web/packages/rwunderground/index.html
#library(pacman)
#p_load("rwunderground")

#Interesting functions: geolookup
# geolookup(set_location(territory = "ES", city = "Moia"))
#>Please enter your weather underground API key and press enter:
#>Error: Invalid key!
#
# They require an API key, and distontinued the API Key free service in 2018
# https://apicommunity.wunderground.com/weatherapi/topics/end-of-service-for-the-weather-underground-api
# ---
#
# Therefore, web scraping will be the workaround to get some of their valuable data
if (FALSE) {
  
  anys <- c(2014:2018)
  mesos <- c("01", "02", "03", "04", "05", "06", "07", "08", "09", "10", "11", "12")
  wu.url <- data.frame()
  for (any in 1:length(anys)) {
    for (mes in mesos[1]:mesos[length(mesos)]) {
      wu.url[mes, any] <-
        paste0("https://www.wunderground.com/history/monthly/es/sant-cugat-del-vall%C3%A8s/LEBL/date/", anys[any], "-", mesos[mes], "?cm_ven=localwx_history")
    }
  }
  wu.url
  p_load("rvest", "dplyr")
  require(methods)
  require(rvest)
  my.rda.file <- "last.wu.data.Rda"
  if (file.exists(my.rda.file)) {
    load(file=my.rda.file)
    wu.data.all.previous <- wu.data.all
  } else {
    wu.data.all.previous <- NULL
  }
  url_base <- wu.url
  
  webpage <- list()
  html <- list()
  ii <- 0
  if (exists("wu.data")) rm(wu.data); wu.data <- list()
  for (any in 1:length(anys)) {
    for (mes in mesos[1]:mesos[length(mesos)]) {
      # download html files
      any<-1
      mes<-1
      ii <- ii+1
      html[[ii]] <- read_html(wu.url[mes, any])
      webpage[[ii]] <- list(mes=mes,
                            any=any,
                            html=html[[ii]])
      
      # the data we want is in the first table on this page
      # the html_table() command coerces the data into a data frame
      
      webpage[[ii]]$html %>%
        html_nodes("table.days") 
      
      # Fetch data
      wu.data[[ii]]$html <- webpage[[ii]] %>%
        html_nodes("table") %>%
        .[[1]] %>%
        html_table() 
      
      # tsting
      html_nodes(
        html_nodes(
          html_nodes(webpage[[ii]], ".views-field-field-documento")[-1],
          ".file"),
        "a")
      
    }
    
  }
}

```
## Get historical wheather data (rainfall, temp) - by WeeWX powered Catalan weather stations

```{r Get historical wheather data (rainfall, temp) - by WeeWX powered Catalan weather stations}
# WeeWX - Open source software for your weather station. 
# http://www.weewx.com/stations.html
# ----------------------------------
# WeeWX is a free, open source, software program, written in Python, which interacts with your weather station to produce graphs, reports, and HTML pages. It can optionally publish to weather sites or web servers. Thousands of stations throughout the world run weeWX, many of whom have opted-in to be shown on our station map. 
#
# Estacions interessants amb dades de pluja
#
# TERRASSA, estación W8681 de EA3KZ. Estación Meteorológica en Terrassa. Latitud: 41° 34.62' N Longitud: 002° 00.98' E Altura: 300 metros [JN11AN]. La estación meteorológica en uso es una W-8681 conectada a un pequeño ordenador Raspberry pi con el software weewx twitter @ea3kz_wx
# Estació Meteorològica sense fils W8681 PROII. Preu: 220.22 € [transport gratuït, només Espanya península] - https://www.astroradio.com/ca/514040
# https://meteo.ea3kz.com/history.html
# https://meteo.ea3kz.com/NOAA/NOAA-2018-08.txt
# https://meteo.ea3kz.com/NOAA/NOAA-2018-09.txt
# https://meteo.ea3kz.com/NOAA/NOAA-2018-10.txt
# https://meteo.ea3kz.com/NOAA/NOAA-2018-11.txt
# https://meteo.ea3kz.com/NOAA/NOAA-2018-12.txt
p_load("readr")
anys <- c(2015:2018)
dies_mes <- c(31,28,31,30,31,30,31,31,30,31,30,31)
nom_meteo <- "Terrassa"
my.data <- list()
ii <- 0
for (any in 1:length(anys)) {
  for (mes in mesos[1]:mesos[length(mesos)]) {
    ii <- ii+1
    my.file <- paste0("NOAA-", 
                      anys[any],
                      "-",
                      mesos[mes],
                      ".txt")
    
    download.file(paste0("https://meteo.ea3kz.com/NOAA/", my.file),
                  file.path("precipitacio", 
                            paste0(nom_meteo, "-", my.file)
                            )
                  )
    
    my.data[[ii]] <- read_fwf(file.path("precipitacio", 
                                  paste0(nom_meteo, "-", my.file)
                                  ),
                        fwf_cols("DAY"=3, 
                                 "MEAN_TEMP"=7, 
                                 "HIGH"=7, 
                                 "H_TIME"=7, 
                                 "LOW"=7, 
                                 "L_TIME"=7, 
                                 "HEAT_DEG_DAYS"=7, 
                                 "COOL_DEG_DAYS"=7,
                                 "RAIN_mm"=7,
                                 "AVG_WIND_SPEED_kph"=7,
                                 "HIGH_WIND"=7,
                                 "TIME_WIND"=7,
                                 "DOM_DIR"=7
                        ),
                        skip = 13, 
                        n_max = dies_mes[mes],
                        na="-",
                        col_types=NULL
    )
    if (mes!=1 & anys[any]!=2015 & nom_meteo!="Terrassa") {
      stop_for_problems(my.data[[ii]])
      problems(my.data[[ii]])
    }
    
    my.data[[ii]] <- my.data[[ii]] %>%
      mutate(year=anys[any],
             month=mesos[mes],
             site=nom_meteo
      )
  }
}

# El Serrat del Serí, Gurb, Catalunya
# ELEV: 525 metres    LAT: 41-58.44 N    LONG: 002-14.28 E
# http://elserrat.cat/weather/history.html
# http://elserrat.cat/weather/NOAA/NOAA-2018-08.txt
# http://elserrat.cat/weather/NOAA/NOAA-2018-09.txt
# http://elserrat.cat/weather/NOAA/NOAA-2018-10.txt
# http://elserrat.cat/weather/NOAA/NOAA-2018-11.txt
# http://elserrat.cat/weather/NOAA/NOAA-2018-12.txt




```

```{r Get historical wheather data (rainfall, temp) - potential alternative ways}
# AWEKAS (AUstria)
# https://www.awekas.at/
# AWEKAS is an abbrevation for “Automatic WEather map (german: KArten) System”. It is a system that processes indicated values of private weather stations graphically, generates weather maps and evaluates the data.
# Estació de tona, en gràfics, no taula de dades. :-(
# https://www.awekas.at/en/instrument.php?id=10147#day

# Open Weather Map
# https://openweathermap.org/price
# ----------------------------------
# 10 USD for 6 year of historical wheater data for a given city
# https://openweathermap.org/history
# https://openweathermap.org/history-bulk


# Agromonitoring 
# ----------------------------------
# See 
# https://agromonitoring.com/api/history-weather
# https://agromonitoring.com/api/accumulated-precipitation
# However historical rainfall is for paid subscription palnms only
# https://agromonitoring.com/price

# Worldbank
# ---------------------------------
# http://sdwebx.worldbank.org/climateportal/index.cfm?page=downscaled_data_download&menu=historical
# They allow downloading some excel file from any country with temp or rainfal from several periods, last one available by the time of this writing is 1991-2015 (better than nothing for my use case :-) 

```


## Save extended results data set as Spreadsheet and csv (just in case)

```{r Save extended results data set as Spreadsheet and csv (just in case)}
#str(md)
results.fname.noext <- paste0("_120000_", format(Sys.Date(), "%y%m"), "00_data")
readr::write_csv(md, file.path(path.data, paste0(results.fname.noext,".csv")))
openxlsx::write.xlsx(md, file = file.path(path.data, paste0(results.fname.noext, ".xlsx")))

```

```{r save DT html}
# ------
  # Add the data table next to the map
dt.data <- datatable(md,  filter = 'top', extensions="Scroller", 
                     style="bootstrap", class="compact",
                     width="100%", rownames = FALSE, 
                     options=list(deferRender=TRUE, 
                                  scrollY=300, 
                                  scroller=TRUE, 
                                  pageLength = 5)) %>%
  formatDate('Date', 'toLocaleDateString') %>%
  formatStyle('Abundància', 
              color = styleInterval(c(3, 5), c('black', 'red', 'blue')),
              backgroundColor = styleInterval(c(3, 5), c('white', 'lightyellow', 'yellow'))
  )


#saveWidget(ct.map.table.all, file=file.path(getwd(), path.html, "ct.map.table.all.html"), selfcontained=TRUE) 

# Save chart as html on disk
htmltools::save_html(dt.data, 
                     file.path(getwd(), path.html, "table.all.html")
                     )

```


## Pivot table

```{r pivot table, echo=FALSE}

# Create the pivot table and save html on disk
#rpivotTable(md)

pt.md <- rpivotTable(
  data = md, 
  rows = c("nom vulgar (comestible)"), 
  cols=c("Year", "Month"), 
  aggregatorName = "Count", 
#  aggregatorName = "Sum", 
#  vals = "Abundància", 
  width="1200px", 
  height="800px",   
#  inclusions = list( Abundància = list("2")),
  exclusions = list( `nom vulgar (comestible)` = list( "---")),
  rendererName = "Heatmap"
  )
frameWidget(pt.md)
saveWidget(pt.md, file=file.path(getwd(), path.html, "boleteRa.pivotTable.html"), selfcontained=TRUE) 

#pt.md


```

## Thematic map

See: https://github.com/mtennekes/tmap

```{r Thematic map, echo=FALSE}
  # =======================================================
  # Use the thematic maps package
  # See: https://github.com/mtennekes/tmap
  # =======================================================
  # 
  # Code will come here


```

## Crosstalk with filters

Taken from https://rstudio.github.io/crosstalk/using.html

```{r Crosstalk with filters short version, echo=FALSE}
#Reset DAte as posixct instead of posixlt
md$Date <- as.POSIXct(md$Date, format = "%m/%d/%Y %H:%M")

#Filter subset of md with data in species
md.sample <- md %>%
  select(
    Any=Year,
    Mes=Month,
    Abundància,
#    Data=Date,
    NomC=`nom vulgar (comestible)`,
#    NomNC=`nom vulgar (no comestible)`,
#    DetallBolet=`Detalls del bolet`,
    DescripcioLloc=`descripció lloc`,
    Lat=Latitude,
    Long=Longitude) %>%
  filter(!is.na(NomC)) %>%
  filter(NomC != "---") %>%
  filter(NomC != "altres") %>%
  top_n(15)

sdf <- SharedData$new(md.sample, as.character(md.sample$NomC))


bscols(widths = c(3,NA,NA),
  # Col 1: Filters
    list(
      filter_checkbox("Ab", "Abundància", sdf, ~Abundància, inline = TRUE),
      filter_slider("A", "Any", sdf, column=~Any, step=1, width="100%"),
      filter_slider("M", "Mes", sdf, column=~Mes, step=1, width="100%")
#   , filter_select("auto", "Automatic", shared_mtcars, ~ifelse(am == 0, "Yes", "No"))
    ),
  # Col 2: Map (LeafLet)
  leaflet(sdf) %>% addTiles() %>% addMarkers(
    label = paste0(md.sample$Any, "-", md.sample$Mes, " ", as.character(md.sample$NomC)),
    popup = paste(
      "Name1:", md.sample$NomC, "<br>",
      "Abundance:", md.sample$Abundància, "<br>",
      "Date:", paste0(md.sample$Any, "-", md.sample$Mes), "<br>"
#      ,"Mushroom Details:", md.sample$DetallsBolet, "<br>",
#      "Forest density:", md.sample$Densitat, "<br>",
      ,"Site desc:", md.sample$DescripcioLloc, "<br>"
#                 "Slope:", md.sample$Pendent, "<br>",
#                 "Picture:", md.sample$foto, "<br>",
      )),
  # Col 3: DT
  datatable(sdf, extensions="Scroller", style="bootstrap", class="compact", width="100%",
            options=list(deferRender=TRUE, scrollY=300, scroller=TRUE)
            )
  )
    
```


```{r Crosstalk with filters long version, echo=FALSE}

# Initialize basemap for r package leaflet
tilesURL <- "http://server.arcgisonline.com/ArcGIS/rest/services/Canvas/World_Light_Gray_Base/MapServer/tile/{z}/{y}/{x}"

# See leaflet marker options here: 
# https://rstudio.github.io/leaflet/markers.html 

# Create the Leaflet map with my data md (ll.md)
  ll.md <- leaflet::leaflet(md) %>% 
      # Base groups
    addProviderTiles(providers$OpenStreetMap, group = "OSM") %>%
#    addTiles(tilesURL, 	attribution='Map tiles by <a href="http://server.arcgisonline.com">ArcGIS online</a> &mdash; Map data &copy; <a href="http://www.openstreetmap.org/copyright">OpenStreetMap</a>') %>% 
  
    # Overlay groups
    addMarkers(label = ~as.character(md$`nom vulgar (comestible)`),
               popup = paste(
                 "Name1:", md$`nom vulgar (comestible)`, "<br>",
                 "Name2:", md$`nom vulgar (no comestible)`, "<br>",
                 "Abundance:", md$Abundància, "<br>",
                 "Mushroom Details:", md$`Detalls del bolet`, "<br>",
                 "Forest density:", md$`Densitat del sotabosc`, "<br>",
                 "Site desc:", md$`Descripció del lloc`, "<br>",
                 "Slope:", md$Pendent, "<br>",
                 "Picture:", md$foto, "<br>",
                 "Date:", md$Date
                 ),
               clusterOptions = markerClusterOptions(),
               group = "Edible") %>%

    # addMarkers(label = ~as.character(d$`nom vulgar (no comestible)`),
    #            popup = ~as.character(d$Abundància),
    #            clusterOptions = markerClusterOptions(),
    #            group = "Non Edible") %>%
  
    # Layers control
  addLayersControl(
    baseGroups = c("OSM"),
#    overlayGroups = c("Edible", "Non Edible"),
    overlayGroups = c("Edible"),
    options = layersControlOptions(collapsed = FALSE)
  ) %>%
  
    # Legend
    addLegend("bottomright", colors= "blue", labels="MyFindings", title="Catalonia")
  
# ------
# Use SharedData like a dataframe with Crosstalk-enabled widgets
# ct stands for CrossTalk
ct.map.table.all <- bscols(
  # Draw the Leaflet map from my data md
  ll.md,

  # Add the data table next to the map
  datatable(md[5:10],  filter = 'top', extensions="Scroller", 
            style="bootstrap", class="compact",
            width="100%", rownames = FALSE, 
            options=list(deferRender=TRUE, 
                         scrollY=300, 
                         scroller=TRUE, 
                         pageLength = 5)) %>%
    formatDate('Date', 'toLocaleDateString') %>%
    formatStyle('Abundància', 
                color = styleInterval(c(3, 5), c('black', 'red', 'blue')),
                backgroundColor = styleInterval(c(3, 5), c('white', 'lightyellow', 'yellow'))
              )
)

#saveWidget(ct.map.table.all, file=file.path(getwd(), path.html, "ct.map.table.all.html"), selfcontained=TRUE) 

# Save chart as html on disk
htmltools::save_html(ct.map.table.all, 
                     file.path(getwd(), path.html, "map.table.all.html")
                     )

webshot(file.path(getwd(), path.html, "map.table.all.html"),
        file=file.path(getwd(), path.html, "map.table.all.png"),
        delay=0.5)

ct.map.table.all

```

![My Map - overview](./html/map.table.all.png)


